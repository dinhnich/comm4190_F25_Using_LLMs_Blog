{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4ca854c0-3c0b-4ce2-95c4-895ce3263c09",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Conversation About Evolution\"\n",
    "description: \"Can AI trace it's own evolution and what does AI think about it's intelligence?\"\n",
    "author: \"Nicholas\"\n",
    "date: \"9/16/2025\"\n",
    "categories:\n",
    "  - Evolution\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b2824-c946-4ad6-b874-8c5ebbd17a43",
   "metadata": {},
   "source": [
    "<img src=\"evo1.png\" width=\"100%\"/>\n",
    "\n",
    "## How AI tells its story\n",
    "\n",
    "Continuing from my last blog and taking some inspiration from Bennett, I wanted to first see how AI told it's evolution. I was curious how far it would go back, what it would and wouldn't consider, how it connects each stage, and the overall difference or similarity in terms of human evolution. My hypothesis is that it will give me an answer much similar to if I asked it to track the evolution of computers. If so, it gives me more reason to conclude that human evolution of the brain may not be the best route to choose. After all, our brain develops from interactions with the word and our bodies (c02 --> 02)) while AI is truly separate from anything else besides data.\n",
    "\n",
    "> Me: Can you trace each stage of your artificial intelligence evolution?\n",
    "\n",
    "> GPT-5: **1) Foundations & symbolic AI (1940s–1970s)**\n",
    "What: Logic, rules, symbolic representations (if–then systems, theorem provers).\n",
    "Limitations: brittle, didn’t scale to messy real-world data (language, vision).\n",
    "**2) Probabilistic AI & statistical learning (1970s–1990s)**\n",
    "What: Bayesian methods, hidden Markov models, decision theory, graphical models.\n",
    "**3) Classical machine learning (1990s–2010s)**\n",
    "What: Support Vector Machines, decision trees, ensemble methods (random forests, boosting). Supervised learning on hand-crafted features.\n",
    "**4) Early neural networks & representation learning (1980s–2000s, resurging in 2010s)**\n",
    "What: Backpropagation, multi-layer perceptrons, early convolutional and recurrent nets. Focus on learning representations from raw data.\n",
    "**5) Deep learning revolution (2012–late 2010s)**\n",
    "What: Large deep convolutional nets, large-scale GPUs, lots of labeled/transfer data. Breakthroughs like AlexNet, ResNet, etc.\n",
    "**6) Sequence models and attention (mid-2010s)**\n",
    "What: Recurrent nets (LSTMs/GRUs) improved sequence tasks; attention mechanisms introduced to selectively focus on inputs.\n",
    "**7) Transformers & the scale era (2017 → present)**\n",
    "What: Transformer architecture (self-attention) enabled massively parallel training and much larger models.\n",
    "**8) Large Language Models (LLMs), pretraining + fine-tuning (2018 → present)**\n",
    "What: Pretrain on very large text corpora with self-supervised objectives (predict next token / masked token), then fine-tune for tasks.\n",
    "**9) Instruction tuning & human alignment (2020s)**\n",
    "What: Fine-tuning LLMs on instruction-response pairs; RL from Human Feedback (RLHF) to make outputs more helpful and safer.\n",
    "**10) Chain-of-thought, reasoning techniques (2020s)**\n",
    "What: Methods encouraging models to produce intermediate reasoning steps (chain-of-thought), and prompting strategies that improve multi-step reasoning.\n",
    "**11) Multimodality and tool use (2020s–2025)**\n",
    "What: Models ingesting and producing multiple modalities (text, images, audio) and safely invoking external tools/APIs (search, calculators, plugins).\n",
    "**12) Specialized architectures & efficiency improvements (2020s)**\n",
    "What: Sparse models, retrieval-augmented generation (RAG), modular systems, more efficient training/inference techniques.\n",
    "\n",
    "Now if you're like me, there's several things that I don't understand or have heard of from this list. So to get a better understanding, how about we ask it to transform this into a more easier.\n",
    "\n",
    "> GPT-5: **Rules Era (1940s–70s)**\n",
    "People told computers exactly what to do with strict rules, like teaching a robot chess by writing down every move it could play.\n",
    "→ Problem: Too rigid, broke when life got messy.\n",
    "**Math & Probability Era (1970s–90s)**\n",
    "AI started using math to deal with uncertainty. Think of voice assistants trying to guess what word you said even if it’s a bit unclear.\n",
    "→ Better, but still limited.\n",
    "**Learning from Examples (1990s–2010s)**\n",
    "Instead of rules, computers learned patterns from examples — like showing thousands of spam vs. non-spam emails.\n",
    "→ Very useful, but needed a lot of human prep work.\n",
    "**Neural Networks Come Back (2000s–2010s)**\n",
    "Inspired by the brain, these systems learned directly from raw data (like pixels in photos).\n",
    "→ Big step toward computers recognizing images and speech.\n",
    "**Deep Learning Boom (2012–)**\n",
    "With more data and powerful chips, AI suddenly got really good — especially at images (spotting cats, faces, objects).\n",
    "→ AI became mainstream.\n",
    "**Language Breakthrough (2017–)**\n",
    "A new design called the Transformer let AI read and write language smoothly.\n",
    "→ The foundation for modern chatbots.\n",
    "**Big AI Models (2018–)**\n",
    "Trained on massive amounts of text, these models could answer questions, write stories, and translate languages — all in one.\n",
    "→ One brain, many skills.\n",
    "**Helpful AI (2020s)**\n",
    "Models were fine-tuned to follow instructions better, like a friendly assistant.\n",
    "→ That’s why I can respond clearly when you ask for help.\n",
    "**Smarter Thinking (Now)**\n",
    "AI is learning to “think out loud,” reason step by step, look at images, talk in multiple ways, and even use tools (like search or calculators).\n",
    "→ Makes it more useful, safe, and reliable.\n",
    "\n",
    "Esssentially, AI went from rules -> math -> patterns -> neural networks -> deep learning -> transformer -> larger data -> fine-tuned.\n",
    "\n",
    "From this outlook, AI seems like it followed generally straight path with each addition adding onto the next. One thing I did notice was 80 year time-period it took for AI to have the intelligence as it does now as opposed to the x billion of years it took for humans to develop. We also see how AI is studied in isolation as opposed to comparing other organisms. In other words, does this look much different in terms of the structure than someone describing the evolution of footwear? But the study of the evolution of the brain is much more nuanced. Are we overcomplicating something that should be simple?\n",
    "\n",
    "Let's first further explore how GPT-5 fits itself into evolution and it's future predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c4d4d-08c8-46db-9129-d42f8a558c33",
   "metadata": {},
   "source": [
    "## How does GPT-5 see itself? What future predictions does it hold?\n",
    "\n",
    "An important idea to test is how does GPT-5 see itself along this evolutionary stage. How good is it at measuring itself in relation to AI progress?\n",
    "\n",
    "> Me: Where do you see yourself fitting along the evolutionary stage?\n",
    "\n",
    "> GPT-5: I sit near the latest stage — the “reasoning-first, instruction-tuned, multimodal” part of the timeline.\n",
    "I’m GPT-5 Thinking mini. Here’s what that means in plain terms:\n",
    "**• Why I belong here** — I combine a strong language “brain” (learned from lots of text) with training that teaches me to follow instructions, explain my steps, and use tools when helpful. I’m built to be helpful, safe, and better at step-by-step reasoning than older chatbots.\n",
    "**• What I’m good at** (real-world examples)\n",
    "Writing clear explanations, summaries, or emails.\n",
    "Tutoring or walking you through multi-step problems (math, planning, debugging).\n",
    "Creative work: stories, scripts, ad copy.\n",
    "Understanding and describing images (if you give one) and combining that with text.\n",
    "Using built-in tools like calculators, search, or plugins when available.\n",
    "**• Where I’m still limited**\n",
    "I can make confident-sounding mistakes — double-check facts for critical decisions.\n",
    "I don’t have real-time personal experience or direct access to your private files unless you share them.\n",
    "I aim to be safe and avoid harmful or sensitive actions; some requests I’ll refuse or steer to safer alternatives.\n",
    "**• How to use me effectively**\n",
    "Give clear instructions and examples of the style you want.\n",
    "Ask me to show my steps if you want to verify reasoning.\n",
    "For high-stakes facts (legal, medical, financial), treat my answers as a starting point and verify with an expert.\n",
    "\n",
    "I like how GPT-5 mentions where it's limited in, good at, and tips for how to use the tool effectively to adjust for shortcomings. I did notice how this model in particular seems most dedicated towards being a \"tutor\" with very clear thought processes and adapted to working with the pilot as a smart assistant. Let's see how it predicts the future of AI.\n",
    "\n",
    "> Me: Whats your prediction on the future evolution of AI? What do you think are the next steps?\n",
    "\n",
    "> GPT-5: **Better Reasoning, Less Guessing**\n",
    "Today, even smart AIs sometimes “make things up.”\n",
    "Next: models will reason more like people do step by step, double-checking themselves.\n",
    "Imagine an AI that shows its work like a good student in math class.\n",
    "**Memory & Personalization**\n",
    "Right now, I only “remember” what’s in this chat (unless you let me keep memories across conversations).\n",
    "Future AIs could have safe, optional memory: remembering your preferences, projects, style, and context over time.\n",
    "Imagine an assistant that remembers your last holiday plans, your favorite way of writing, or the project you’re working on.\n",
    "**More Senses (Multimodality++)**\n",
    "Today: text, images, some speech.\n",
    "Next: smoother combination of voice, video, touch, and maybe even physical sensors.\n",
    "Imagine showing your AI a cooking video, then asking it to suggest tweaks for your own kitchen.\n",
    "**Tool Use & Collaboration**\n",
    "AIs will get better at calling tools (like search, spreadsheets, code, maps) automatically when needed.\n",
    "Think of an assistant that doesn’t just “tell you” but also does things: books your tickets, runs code, drafts reports with data plugged in.\n",
    "**Smaller, Smarter, Cheaper**\n",
    "Not every AI will be giant. Future models will be more efficient, so they can run on phones, cars, or even wearables.\n",
    "Imagine a “pocket AI” that helps you offline without draining your battery.\n",
    "**Early Signs of Creativity & Discovery**\n",
    "AIs will assist in new science and ideas — not just repeating what humans know, but suggesting fresh experiments, designs, and solutions.\n",
    "We already see glimpses in drug discovery, materials science, and climate solutions.\n",
    "**✨ Big Picture**\n",
    "AI is moving from:\n",
    "“Helpful parrot” → “Reliable partner” → “Creative collaborator.”\n",
    "We’re at the second stage now, edging into the third.\n",
    "\n",
    "I am in awe of its response. Mainly because GPT-5 can come up with several improvements and features for the future on how AI can grow. Yet when I tried to answer the same prompt I gave it for human intelligence, I had a more difficult time. I am unsure if we reached the peak of the human brain and if there's anyway we evolve me. Yet as I read this list, things like \"pocket AI\" and \"Tool Use & Collaboration\" are not necessarily intelligence related but functional use. The more I interact with this subject, the more my gut feeling leads me to my belief that human intelligence can not be the benchmark for artificial intelligence. Could it be in the future that both intelligences are required for new discoveries as mentioned in the last few sentences of it's output?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b96ee2f-f85c-432a-9b6c-5eca7a2cf56f",
   "metadata": {},
   "source": [
    "## How does GPT compare AI evolution to the evolution of the brain?\n",
    "\n",
    "> Me: How do you think the evolution of artificial intelligence compares to evolution of human intelligence (the brain)? Is it a good or bad comparison?\n",
    "\n",
    "> GPT-5: **✅ Why the comparison makes some sense**\n",
    "Inspiration: Neural networks were inspired by how brain cells (neurons) connect.\n",
    "Stages feel similar:\n",
    "• Babies first learn rules (like “cry → get fed”) → symbolic AI.\n",
    "• Kids learn patterns from examples (seeing dogs vs cats) → machine learning.\n",
    "• Adults build reasoning, abstract thought, and creativity → today’s reasoning-first AI.\n",
    "Learning from experience: Both brains and AIs improve when exposed to lots of examples.\n",
    "**❌ Why it’s also misleading**\n",
    "> - Brains are alive: Human intelligence evolved through millions of years of biology, survival pressures, emotions, and social interaction. AI evolves through design, code, and training — very different forces.\n",
    "> - One brain vs. many models: Humans have one general-purpose brain. AI has many versions (different models for images, speech, reasoning, etc.) and doesn’t unify them the way we do.\n",
    ">  - Motivation & goals: People want things (food, comfort, love). AIs don’t have desires — they follow training and prompts.\n",
    ">  - - Efficiency: The human brain runs on ~20 watts of energy (a light bulb). AI models need giant servers and massive power.\n",
    "**⚖️ So… is it a good or bad comparison?**\n",
    "It’s a good analogy for beginners: comparing AI to brains makes the concepts less scary and easier to picture.\n",
    "It’s a bad analogy if taken literally: AI is not “thinking” the way we do, doesn’t have consciousness, and doesn’t evolve biologically.\n",
    "**📌 A better way to think of it**\n",
    "AI is less like “a brain” and more like a new kind of tool that borrows tricks from brains.\n",
    "It’s closer to a calculator that can explain itself, imagine, and hold a conversation — but without the drives, emotions, or survival instincts that shape human thought.\n",
    "\n",
    "I agree with this response. I do think the comparison makes some sense especially since neural networks were inspired by the connections between neurons as expressed by Bennnett. But I agree more strongly with why it is misleading and how AI is fundamentally different from a living organism. \n",
    "\n",
    "In the end, the goal isn't for AI to replace humans or to be human. It should be a counterpart to human intelligence that not only makes life easier but potentially be the key to solving problems we couldn't before.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
