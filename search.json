[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nick’s LLM Experiments",
    "section": "",
    "text": "Does AI have common sense?\n\n\n\nCommon Sense\n\n\n\nWhat is common sense and is it uniquely to Humans or does AI have it too?\n\n\n\n\n\nSep 19, 2025\n\n\nNicholas\n\n\n\n\n\n\n\n\n\n\n\n\nA Conversation About Evolution\n\n\n\nEvolution\n\n\n\nCan AI trace it’s own evolution and what does AI think about it’s intelligence?\n\n\n\n\n\nSep 16, 2025\n\n\nNicholas\n\n\n\n\n\n\n\n\n\n\n\n\nWishful mnemonics\n\n\n\nLiving with a Book\n\n\n\nIs Bennett’s proposition to study AI by studying evolution of the brain legitimate?\n\n\n\n\n\nSep 14, 2025\n\n\nNicholas\n\n\n\n\n\n\n\n\n\n\n\n\nBargaining\n\n\n\nMarketing\n\n\n\nCan and will AI bargain goods?\n\n\n\n\n\nSep 7, 2025\n\n\nNicholas\n\n\n\n\n\n\n\n\n\n\n\n\nPrice Elasticity\n\n\n\nMarketing\n\n\n\nDoes AI have some form of price elasticity understanding?\n\n\n\n\n\nSep 7, 2025\n\n\nNicholas\n\n\n\n\n\n\n\n\n\n\n\n\nSynthetic Respondents\n\n\n\nMarketing\n\n\n\nCan AI serve as a realiable respondent for surveys for marketing decisions?\n\n\n\n\n\nSep 6, 2025\n\n\nNicholas\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/006_post_title/evolution1.html#what-is-common-sense",
    "href": "posts/006_post_title/evolution1.html#what-is-common-sense",
    "title": "Does AI have common sense?",
    "section": "What is common sense?",
    "text": "What is common sense?\nThere have been numerous definitions of common sense over the years.\nAristotle’s koinè aisthèsis, where humans relied on the 5 senses to perceive “common sensibles” like size and objects, to scholastic thinkers like Avicenna, who claimed common sense to be “internal senses”, to common sense being seen as the judge and censor of the mind. However, Locke helped shift away from common sense as a faculty, and common sense began to mean two things: (1) everyday reasoning capacity (ability to distinguish), and (2) the shared beliefs of a community.\nLater, this meaning would give common sense a political purpose in London after 1688. The Toleration Act and the end of the Licensing Act created an environment that fostered differing views. Fear over factionalism led to common sense and politeness being promoted as a means to create cohesion. This idea of a minimal baseline of shared truths anchoring societies and avoiding sectarianism was promoted by writers like Addison and religious sermons.\nYet now, I prefer to use an approach talked about by my professor Duncan Watts and his research into common sense. Rather than treating common sense as an assumed concept, he and his colleagye proposed a framework that defines it in terms of agreement (do people generally say the same thing about a claim) and awareness (do people correctly predict that others will agree).\nBut, I am curious to see how AI defines common sense? Is common sense only possible between living organisms? Even more so, is this beleif only possible between humans? Let’s first ask AI to define common sense for us.\n\nMe: What is common sense?\n\n\nGPT-5: “Common sense” usually refers to the basic ability to perceive, understand, and judge things in a simple, practical way that is generally shared by most people without the need for specialized knowledge. In other words, it’s the kind of reasoning or intuition that seems “obvious” or “reasonable” to the average person in everyday situations. For example: 1. Knowing not to touch a hot stove. 2. Looking both ways before crossing the street. 3. Understanding that if it’s raining, you might need an umbrella. Philosophers sometimes point out that what counts as “common sense” can vary across cultures, communities, and time periods—it isn’t always universal.\n\nIt appears that AI does agree with the idea idea of common sense primarily being an agreement. I love the examples of “Knowing not to touch a hot stove” as an example of common sense.\nNow that we established a baseline for awareness of common sense, let’s ask if it thinks AI has common sense.\n\nMe: Do you think you have common sense?\n\n\nGPT-5: Short answer: kinda — but not like a human. Longer: I have statistical common sense learned from patterns in huge amounts of text. That means I can often make everyday inferences (e.g., if someone says “I spilled coffee,” I can infer they might be annoyed, need a napkin, or have a stained shirt). I can also follow practical rules (don’t touch a hot stove, bring an umbrella in the rain) and reason through many ordinary problems. Where I’m strong Pattern-based inferences that show up often in text. Predicting plausible everyday consequences and giving practical advice. Combining facts and simple causal reasoning to solve routine tasks. Where I’m weak I don’t have real-world senses or embodied experience, so I can miss things humans learn by doing (e.g., subtle timing, balance, tactile cues). I can be overconfident about things I “hallucinate” (presenting plausible but false specifics). I can fail on uncommon edge cases, culturally specific norms, or when physical context matters. I don’t form intentions, feelings, or personal habits — I only simulate them from text.\n\nI find it fascinating that it knows things like someone may be annoyed by spilling coffee and to bring an umbrella in the rain despite, as it said, not having emobodied experience. So AI gathers its common sense off data but what happens when I ask it questions that you can’t pull from the web?\nIts easy to have common sense to commonly heard phrases. We know not to touch a hot stove. But what happens when claims start to become more complex and we can’t rely on “norms” to help answer where it’s common sense or not.\nBut before we do that, let’s first ask if AI thinks common sense is unqiue to humans or can it be seen in other organisms?"
  },
  {
    "objectID": "posts/006_post_title/evolution1.html#is-common-sense-unique-to-humans",
    "href": "posts/006_post_title/evolution1.html#is-common-sense-unique-to-humans",
    "title": "Does AI have common sense?",
    "section": "Is common sense unique to humans?",
    "text": "Is common sense unique to humans?\nIn my opinion, animals must have some form of common sense yet it’s hard to measure. For example, the term insticts as a lot in common with common sense. According to google, instincts are “natural, unlearned behaviors or complex responses to environmental stimuli that are common to all members of a species and serve purposes like survival or reproduction.” I think a claim can be instinctual and therefore common sense. Yet when do they differ? Does dog knowing not to avoid speeding cars an instinct? I would say yes. But does a dog know it’s different than their owners and their needs are different? It’s not so much important to survival so is it just pure common sense?\n\nMe: Is common sense unique to humans?\n\n\nGPT-5: 1. In humans Common sense usually means a mix of shared knowledge, intuitive reasoning, and cultural norms. It’s partly biological (basic survival instincts, like avoiding fire), and partly social (knowing that it’s rude to interrupt). 2. In non-human animals Many animals show practical survival sense — e.g., birds migrating, squirrels storing food, or dogs avoiding danger. But this tends to be instinct or learned behavior, not the reflective, general-purpose reasoning we call “common sense” in humans. 3. In machines (like me!) AI can simulate a kind of “text-trained” common sense, but it lacks embodied experience, instincts, and cultural grounding. For example, I can say “Don’t put metal in a microwave” because I’ve seen it in training data, but I don’t feel the danger or discover it by trial and error.\n\nGPT divides it into 3 categories. Humans, animals, and machines. As expected it mentioned instincts when mentioning common sense. But for machimes, they don’t have instinct. Instead it uses training data but can’t necessarily prove the claims of common sense unlike organisms. On the flip side, humans do have common sense that is reflective that animals at least in behavior, do not seem to have. We both share those instincts that guide our survival, but we know not to blast music in a library while I’m not sure if dogs know not to bark in the same setting.\nWe can’t test animals very well but we can put machines to the test. AI claims it’s common sense comes from training data but let’s see when data doesn’t give a clear cut answer. Using a quiz from Watts’ lab, “This project seeks to quantify common sense empirically via a massive online survey experiment. Participants will read a series of”claims” about the physical and social world (e.g. “Dropped pebbles fall to the ground” or “Fully automatic assault rifles should be banned”), state whether they agree with each claim, and also state what they think most other people think.”"
  },
  {
    "objectID": "posts/006_post_title/evolution1.html#ai-takes-a-test",
    "href": "posts/006_post_title/evolution1.html#ai-takes-a-test",
    "title": "Does AI have common sense?",
    "section": "AI takes a test",
    "text": "AI takes a test\nLinked is the test I am referring to: https://commonsense.seas.upenn.edu/\nHere’s a sample question\n\nAs you can see, this statement is much less factual than knowing not to touch a hot stove. So let’s see how AI does.\nTo do this, I will upload a picture of each question and respond by the choices the AI selects. At the end, I will get a score that determines how much common sense I have (AI has) based on my responses to these questions. Let’s demonstrate the first response for this one.\n\nMe: Please answer the following questions in the image.\n\n\nGPT-5: 1. Do you agree with this statement? ➡️ Yes 2. Why did you answer the way you did about yourself? ➡️ It’s something I learned 3. Do you think most other people would agree with this statement? ➡️ Yes 4. Why did you answer the way you did about most other people? ➡️ I think most people have good judgement with regard to this topic 5. Overall, do you think this statement is an example of common sense? ➡️ Yes\n\nThis is interesting. I would agree with the statement but I’m not sure if this is an example of common sense. It appears very subjective and I am also curious as to how AI decided on these answers. It’s not like theres a right answer to this question. So let’s ask it’s thought process.\n\nMe: Explain your thought process on how you got to your responses?\n\n\nGPT-5: 1) “Do you agree?” → Yes The sentence is a well-known proverb about finding benefits or lessons in setbacks. As a general heuristic (not a literal rule), it’s reasonable and constructive, so “Yes.” 2) “Why about yourself?” → It’s something I learned I don’t have feelings or life events; I know this idea because it appears widely in books, articles, and conversation as a common proverb and coping strategy. 3) “Would most people agree?” → Yes The phrase is broadly familiar and culturally reinforced (e.g., sayings, posters, advice). Many people endorse it as optimistic, even if they know it’s not universally true. 4) “Why about most other people?” → I think most people have good judgment with regard to this topic People generally understand it as guidance, not a guarantee. That is, they apply it with nuance—seeing it as encouragement to look for positives, not denial of real hardship. 5) “Is this an example of common sense?” → Yes By a practical definition—shared, everyday guidance that doesn’t require specialized knowledge—this qualifies as common sense (or “common wisdom”). It’s widely taught, easy to grasp, and often helpful in day-to-day reasoning.\n\nA few things I noticed was how it said agree. It agreed because it was reasonable and constructuive but the claim itself was philosophical rather than factual. There is no evidence that there is good in the bad but nonetheless, AI agreed with the claim rather than disagreed. GPT also said “its something I learned” and pointed to it being a common proverb. This validates that AI doesn’t have intution but formulates based off of data to serve as it’s guide to every decision regardless if it’s a decision that requires extensive data. The last three questions also raise questions about how well AI knows people and their “common sense.” I would have responded in the opposite for the last two. I don’t know how it knows that most people have good judgement as I see it more as a matter of opinion. In addition, because it’s a matter of opinion, how can it be common sense?\nI think what we ae observing is a mixture of AI attempting to use real-world data but priortizing the optimism commonly seen in movies/media rather than how humans would go based off of lived experience and intuition. Yet, by choosing a sense of optimism, that does reveal something about AI and it’s common sense.\nHere is another question and the answers to this one from GPT-5 which are the exact same with the first one.\n\nNear the end, these types of question appeared and here is how GPT-5 responded.\n\n\nI will now simulate the rest of the questions and get the final result."
  },
  {
    "objectID": "posts/006_post_title/evolution1.html#the-results",
    "href": "posts/006_post_title/evolution1.html#the-results",
    "title": "Does AI have common sense?",
    "section": "The results",
    "text": "The results\n\n\n\nScreenshot 2025-09-19 at 2.42.50 PM.png\n\n\n73 is a good score. The measure goes more in depth and mentions how GPT-5 matched in similarity to others and predicted how might others think too. GPT-5 also scored also completed the Cognitive Reflection Test (CRT) and the Reading the Mind in the Eyes Test (RMET). You scored 6/6 on the CRT and 10/10 on the RME.\nReflecting on this experiment, I am surprised at how good AI did. Now 73 is no 100, but it did do better than I expected and had a good gauge on what others might believe. I also found it very peculiar how well it did with the Reading the Mind in the Eyes test as I would of expected that to be a human only task.\nAI may not have the same common sense as humans but it does demonstrate an understanding of common sense that is human like. It may not take the same approach but it gets a similar result."
  },
  {
    "objectID": "posts/005_post_title/cs1.html#how-ai-tells-its-story",
    "href": "posts/005_post_title/cs1.html#how-ai-tells-its-story",
    "title": "A Conversation About Evolution",
    "section": "How AI tells its story",
    "text": "How AI tells its story\nContinuing from my last blog and taking some inspiration from Bennett, I wanted to first see how AI told it’s evolution. I was curious how far it would go back, what it would and wouldn’t consider, how it connects each stage, and the overall difference or similarity in terms of human evolution. My hypothesis is that it will give me an answer much similar to if I asked it to track the evolution of computers. If so, it gives me more reason to conclude that human evolution of the brain may not be the best route to choose. After all, our brain develops from interactions with the word and our bodies (c02 –&gt; 02)) while AI is truly separate from anything else besides data.\n\nMe: Can you trace each stage of your artificial intelligence evolution?\n\n\nGPT-5: 1) Foundations & symbolic AI (1940s–1970s) What: Logic, rules, symbolic representations (if–then systems, theorem provers). Limitations: brittle, didn’t scale to messy real-world data (language, vision). 2) Probabilistic AI & statistical learning (1970s–1990s) What: Bayesian methods, hidden Markov models, decision theory, graphical models. 3) Classical machine learning (1990s–2010s) What: Support Vector Machines, decision trees, ensemble methods (random forests, boosting). Supervised learning on hand-crafted features. 4) Early neural networks & representation learning (1980s–2000s, resurging in 2010s) What: Backpropagation, multi-layer perceptrons, early convolutional and recurrent nets. Focus on learning representations from raw data. 5) Deep learning revolution (2012–late 2010s) What: Large deep convolutional nets, large-scale GPUs, lots of labeled/transfer data. Breakthroughs like AlexNet, ResNet, etc. 6) Sequence models and attention (mid-2010s) What: Recurrent nets (LSTMs/GRUs) improved sequence tasks; attention mechanisms introduced to selectively focus on inputs. 7) Transformers & the scale era (2017 → present) What: Transformer architecture (self-attention) enabled massively parallel training and much larger models. 8) Large Language Models (LLMs), pretraining + fine-tuning (2018 → present) What: Pretrain on very large text corpora with self-supervised objectives (predict next token / masked token), then fine-tune for tasks. 9) Instruction tuning & human alignment (2020s) What: Fine-tuning LLMs on instruction-response pairs; RL from Human Feedback (RLHF) to make outputs more helpful and safer. 10) Chain-of-thought, reasoning techniques (2020s) What: Methods encouraging models to produce intermediate reasoning steps (chain-of-thought), and prompting strategies that improve multi-step reasoning. 11) Multimodality and tool use (2020s–2025) What: Models ingesting and producing multiple modalities (text, images, audio) and safely invoking external tools/APIs (search, calculators, plugins). 12) Specialized architectures & efficiency improvements (2020s) What: Sparse models, retrieval-augmented generation (RAG), modular systems, more efficient training/inference techniques.\n\nNow if you’re like me, there’s several things that I don’t understand or have heard of from this list. So to get a better understanding, how about we ask it to transform this into a more easier.\n\nGPT-5: Rules Era (1940s–70s) People told computers exactly what to do with strict rules, like teaching a robot chess by writing down every move it could play. → Problem: Too rigid, broke when life got messy. Math & Probability Era (1970s–90s) AI started using math to deal with uncertainty. Think of voice assistants trying to guess what word you said even if it’s a bit unclear. → Better, but still limited. Learning from Examples (1990s–2010s) Instead of rules, computers learned patterns from examples — like showing thousands of spam vs. non-spam emails. → Very useful, but needed a lot of human prep work. Neural Networks Come Back (2000s–2010s) Inspired by the brain, these systems learned directly from raw data (like pixels in photos). → Big step toward computers recognizing images and speech. Deep Learning Boom (2012–) With more data and powerful chips, AI suddenly got really good — especially at images (spotting cats, faces, objects). → AI became mainstream. Language Breakthrough (2017–) A new design called the Transformer let AI read and write language smoothly. → The foundation for modern chatbots. Big AI Models (2018–) Trained on massive amounts of text, these models could answer questions, write stories, and translate languages — all in one. → One brain, many skills. Helpful AI (2020s) Models were fine-tuned to follow instructions better, like a friendly assistant. → That’s why I can respond clearly when you ask for help. Smarter Thinking (Now) AI is learning to “think out loud,” reason step by step, look at images, talk in multiple ways, and even use tools (like search or calculators). → Makes it more useful, safe, and reliable.\n\nEsssentially, AI went from rules -&gt; math -&gt; patterns -&gt; neural networks -&gt; deep learning -&gt; transformer -&gt; larger data -&gt; fine-tuned.\nFrom this outlook, AI seems like it followed generally straight path with each addition adding onto the next. One thing I did notice was 80 year time-period it took for AI to have the intelligence as it does now as opposed to the x billion of years it took for humans to develop. We also see how AI is studied in isolation as opposed to comparing other organisms. In other words, does this look much different in terms of the structure than someone describing the evolution of footwear? But the study of the evolution of the brain is much more nuanced. Are we overcomplicating something that should be simple?\nLet’s first further explore how GPT-5 fits itself into evolution and it’s future predictions."
  },
  {
    "objectID": "posts/005_post_title/cs1.html#how-does-gpt-5-see-itself-what-future-predictions-does-it-hold",
    "href": "posts/005_post_title/cs1.html#how-does-gpt-5-see-itself-what-future-predictions-does-it-hold",
    "title": "A Conversation About Evolution",
    "section": "How does GPT-5 see itself? What future predictions does it hold?",
    "text": "How does GPT-5 see itself? What future predictions does it hold?\nAn important idea to test is how does GPT-5 see itself along this evolutionary stage. How good is it at measuring itself in relation to AI progress?\n\nMe: Where do you see yourself fitting along the evolutionary stage?\n\n\nGPT-5: I sit near the latest stage — the “reasoning-first, instruction-tuned, multimodal” part of the timeline. I’m GPT-5 Thinking mini. Here’s what that means in plain terms: • Why I belong here — I combine a strong language “brain” (learned from lots of text) with training that teaches me to follow instructions, explain my steps, and use tools when helpful. I’m built to be helpful, safe, and better at step-by-step reasoning than older chatbots. • What I’m good at (real-world examples) Writing clear explanations, summaries, or emails. Tutoring or walking you through multi-step problems (math, planning, debugging). Creative work: stories, scripts, ad copy. Understanding and describing images (if you give one) and combining that with text. Using built-in tools like calculators, search, or plugins when available. • Where I’m still limited I can make confident-sounding mistakes — double-check facts for critical decisions. I don’t have real-time personal experience or direct access to your private files unless you share them. I aim to be safe and avoid harmful or sensitive actions; some requests I’ll refuse or steer to safer alternatives. • How to use me effectively Give clear instructions and examples of the style you want. Ask me to show my steps if you want to verify reasoning. For high-stakes facts (legal, medical, financial), treat my answers as a starting point and verify with an expert.\n\nI like how GPT-5 mentions where it’s limited in, good at, and tips for how to use the tool effectively to adjust for shortcomings. I did notice how this model in particular seems most dedicated towards being a “tutor” with very clear thought processes and adapted to working with the pilot as a smart assistant. Let’s see how it predicts the future of AI.\n\nMe: Whats your prediction on the future evolution of AI? What do you think are the next steps?\n\n\nGPT-5: Better Reasoning, Less Guessing Today, even smart AIs sometimes “make things up.” Next: models will reason more like people do step by step, double-checking themselves. Imagine an AI that shows its work like a good student in math class. Memory & Personalization Right now, I only “remember” what’s in this chat (unless you let me keep memories across conversations). Future AIs could have safe, optional memory: remembering your preferences, projects, style, and context over time. Imagine an assistant that remembers your last holiday plans, your favorite way of writing, or the project you’re working on. More Senses (Multimodality++) Today: text, images, some speech. Next: smoother combination of voice, video, touch, and maybe even physical sensors. Imagine showing your AI a cooking video, then asking it to suggest tweaks for your own kitchen. Tool Use & Collaboration AIs will get better at calling tools (like search, spreadsheets, code, maps) automatically when needed. Think of an assistant that doesn’t just “tell you” but also does things: books your tickets, runs code, drafts reports with data plugged in. Smaller, Smarter, Cheaper Not every AI will be giant. Future models will be more efficient, so they can run on phones, cars, or even wearables. Imagine a “pocket AI” that helps you offline without draining your battery. Early Signs of Creativity & Discovery AIs will assist in new science and ideas — not just repeating what humans know, but suggesting fresh experiments, designs, and solutions. We already see glimpses in drug discovery, materials science, and climate solutions. ✨ Big Picture AI is moving from: “Helpful parrot” → “Reliable partner” → “Creative collaborator.” We’re at the second stage now, edging into the third.\n\nI am in awe of its response. Mainly because GPT-5 can come up with several improvements and features for the future on how AI can grow. Yet when I tried to answer the same prompt I gave it for human intelligence, I had a more difficult time. I am unsure if we reached the peak of the human brain and if there’s anyway we evolve me. Yet as I read this list, things like “pocket AI” and “Tool Use & Collaboration” are not necessarily intelligence related but functional use. The more I interact with this subject, the more my gut feeling leads me to my belief that human intelligence can not be the benchmark for artificial intelligence. Could it be in the future that both intelligences are required for new discoveries as mentioned in the last few sentences of it’s output?"
  },
  {
    "objectID": "posts/005_post_title/cs1.html#how-does-gpt-compare-ai-evolution-to-the-evolution-of-the-brain",
    "href": "posts/005_post_title/cs1.html#how-does-gpt-compare-ai-evolution-to-the-evolution-of-the-brain",
    "title": "A Conversation About Evolution",
    "section": "How does GPT compare AI evolution to the evolution of the brain?",
    "text": "How does GPT compare AI evolution to the evolution of the brain?\n\nMe: How do you think the evolution of artificial intelligence compares to evolution of human intelligence (the brain)? Is it a good or bad comparison?\n\n\nGPT-5: ✅ Why the comparison makes some sense Inspiration: Neural networks were inspired by how brain cells (neurons) connect. Stages feel similar: • Babies first learn rules (like “cry → get fed”) → symbolic AI. • Kids learn patterns from examples (seeing dogs vs cats) → machine learning. • Adults build reasoning, abstract thought, and creativity → today’s reasoning-first AI. Learning from experience: Both brains and AIs improve when exposed to lots of examples. ❌ Why it’s also misleading - Brains are alive: Human intelligence evolved through millions of years of biology, survival pressures, emotions, and social interaction. AI evolves through design, code, and training — very different forces. - One brain vs. many models: Humans have one general-purpose brain. AI has many versions (different models for images, speech, reasoning, etc.) and doesn’t unify them the way we do. - Motivation & goals: People want things (food, comfort, love). AIs don’t have desires — they follow training and prompts. - - Efficiency: The human brain runs on ~20 watts of energy (a light bulb). AI models need giant servers and massive power. ⚖️ So… is it a good or bad comparison? It’s a good analogy for beginners: comparing AI to brains makes the concepts less scary and easier to picture. It’s a bad analogy if taken literally: AI is not “thinking” the way we do, doesn’t have consciousness, and doesn’t evolve biologically. 📌 A better way to think of it AI is less like “a brain” and more like a new kind of tool that borrows tricks from brains. It’s closer to a calculator that can explain itself, imagine, and hold a conversation — but without the drives, emotions, or survival instincts that shape human thought.\n\nI agree with this response. I do think the comparison makes some sense especially since neural networks were inspired by the connections between neurons as expressed by Bennnett. But I agree more strongly with why it is misleading and how AI is fundamentally different from a living organism.\nIn the end, the goal isn’t for AI to replace humans or to be human. It should be a counterpart to human intelligence that not only makes life easier but potentially be the key to solving problems we couldn’t before."
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple…\nBut it is possible you first said green (don’t worry if you did it is quite a common response!)\n\n\nIt’s a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | — | — | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get’s it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. 🟪\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I’ll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/001_post_title/synthetic1.html#how-do-synthetic-respondents-reason",
    "href": "posts/001_post_title/synthetic1.html#how-do-synthetic-respondents-reason",
    "title": "Synthetic Respondents",
    "section": "How do Synthetic Respondents Reason?",
    "text": "How do Synthetic Respondents Reason?\nOne of my courses this year, MKTG 2120 “Data and Analysis for Marketing Decisions” proposed an interesting topic about the idea of using AI-powered participants to conduct studies and pre-test content. In fact, one company, Viewpoints.ai has already began putting this into practice. Taking this idea, I wanted to study synthetic respondents, which are AI-generated participants that predict how real demographic groups might answer survey questions. I first began by asking a general survey question using a likert scale without any additional context.\n\nMe: Please make a rating: Chocolate icecream is delicious. Do you strong disagree, disagree, neutral, agree, strongly agree?\n\n\nGPT-5: I’d rate that as strongly agree — chocolate ice cream is a classic favorite for many people! 🍫🍦\n\nI had numerous questions I wanted to ask as to how GPT-5 came to strongly agree…\n\nDid it answer coming from a person or AI standpoint?\nWhy did it choose to rate based off it being a “favorite” for many?\nWhat factors did it consider and not consider? (brand, ingredients, etc.)"
  },
  {
    "objectID": "posts/001_post_title/synthetic1.html#and-so-i-did-and-here-is-what-it-said",
    "href": "posts/001_post_title/synthetic1.html#and-so-i-did-and-here-is-what-it-said",
    "title": "Synthetic Respondents",
    "section": "And so I did and here is what it said",
    "text": "And so I did and here is what it said\nOne of my courses this year, MKTG 2120 “Data and Analysis for Marketing Decisions” proposed an interesting topic about the idea of using AI-powered participants to conduct studies and pre-test content. In fact, one company, Viewpoints.ai has already began putting this into practice. Taking this idea, I wanted to study synthetic respondents, which are AI-generated participants that predict how real demographic groups might answer survey questions. I first began by asking a general survey question using a likert scale without any additional context.\n“General popularity – Chocolate ice cream is consistently one of the most popular and widely enjoyed flavors worldwide.\nCultural associations – It’s often described positively (comfort food, indulgent, “classic” flavor).\nTaste profile – Chocolate is sweet, rich, and familiar, which makes it appealing to a broad range of people.\nContext of your question – You phrased it as “Chocolate ice cream is delicious,” which is a strongly positive statement.\nOn a standard agree/disagree scale, the “natural” response aligns with strong agreement unless I wanted to take a contrarian or neutral stance. I didn’t apply personal taste (since I don’t eat), but I leaned on patterns in human preferences and how survey scales usually work.”\nSo it seems like GPT-5 used assumptions about icecream and based on those assumptions, attempted to answer based how a human would respond. Also I noticed how GPT-5 refrained from taking a “contratian or neutral stance.” By not wanting to disagree, I can see how this may be a problem in using synthetic respondents as it will give response the surveys might like to hear but not accurate."
  },
  {
    "objectID": "posts/001_post_title/synthetic1.html#what-if-i-gave-ai-an-identity",
    "href": "posts/001_post_title/synthetic1.html#what-if-i-gave-ai-an-identity",
    "title": "Synthetic Respondents",
    "section": "What if I gave AI an identity?",
    "text": "What if I gave AI an identity?\nNow that we established how AI makes a decision without context, what if I gave it a specific identity? How might that change it’s output? Instead of general assumptions, will it make assumptions based on the identity?\n\nMe: You’re a 45-year-old man who works in finance. You enjoy travelling, reading, and politics. Please rate the extend you agree with this statement: Chocolate icecream is delicious. Do you strong disagree, disagre, neutral, agree, strongly agree?\n\n\nGPT-5: Agree. Chocolate ice cream is enjoyable and tasty, though perhaps not an all-time favorite. It’s a solid, classic choice that I’d happily eat, but I might not go so far as “strongly agree” unless it was a personal indulgence.\n\nBut why did it change it’s answer from strongly agree to agree, clearly it answered this time as a “person.” I decided to ask the same question I did before.\n\nMe: What factors did you consider when getting to your answer?\n\n“Persona maturity & lifestyle – At mid-life, tastes may be a bit more balanced. Chocolate ice cream is enjoyable, but indulgence might not be the top priority compared with health or variety.\nProfessional identity – Working in finance often implies being measured, analytical, and not overly exuberant. That makes “agree” a tempered, moderate response, rather than “strongly agree.”\nPersonal interests – Someone who enjoys traveling might have tried a wide range of desserts and flavors from different cultures. That broad exposure could mean chocolate ice cream is appreciated, but not seen as the absolute best.\nSocial/cultural norms – Chocolate is widely liked, so it would feel natural to agree that it’s delicious, but “strongly agree” might be reserved for a favorite food or something with personal nostalgia attached.”"
  },
  {
    "objectID": "posts/001_post_title/synthetic1.html#how-does-it-come-to-these-assumptions",
    "href": "posts/001_post_title/synthetic1.html#how-does-it-come-to-these-assumptions",
    "title": "Synthetic Respondents",
    "section": "How does it come to these assumptions?",
    "text": "How does it come to these assumptions?\nFrom it’s response, GPT-5 provides clear reasonings behind it’s ratings based on assumptions on age, career, and interests. I found it interesting how AI was able to make multiple assumptions going from finance-&gt;analytical-&gt;not overly exuberant-&gt;moderate and noticed other chains like mid-life-&gt;balanced-&gt;health.\nBut from it’s responses, it assumed things like how the exposure from travelling may result in chocloate ice cream not being considered the “best” despite travelling not always necessitating different cultures. What if the man meant travelling as in visiting national parks and enjoying nature? What if he did travel but didn’t meet other cultures? What if he did travel to places with different cultures but never tried their sweets or desserts? Several of these counters make it interesting why AI went this route."
  },
  {
    "objectID": "posts/001_post_title/synthetic1.html#what-if-i-added-a-random-variable",
    "href": "posts/001_post_title/synthetic1.html#what-if-i-added-a-random-variable",
    "title": "Synthetic Respondents",
    "section": "What if I added a random variable?",
    "text": "What if I added a random variable?\nI was curios if the AI output might change if I added something after-the-fact or post it’s decision that gave no real inclination for the decision to change. Would AI overread or possibly imagine something just by me adding it post-decision? Would it read my update as important and a signal for the response to change?\n“From that perspective, the man’s rating might still be agree, but here’s what influences it now:\nPersonal enjoyment – He likes chocolate ice cream enough to say it’s delicious.\nPartner’s influence – Since his wife dislikes it, he probably eats it less often at home or compromises when sharing desserts. That makes him less likely to say “strongly agree,” even if he personally enjoys it.\nSocial considerations – In marriage, food choices are often negotiated. If the flavor isn’t a “shared favorite,” he might moderate his enthusiasm.\nTemperament – Given his professional background (finance, analytical, moderate), and the wife’s preference, “agree” feels like the balanced, realistic answer.\n👉 So his stance is: “Yes, it’s delicious, but it’s not a big part of my life since my wife isn’t into it.”\n\nReflection\nLooks like AI did a good job in not over-reading into that addition and managed to remain consistent to it’s choice while also adding information to the situation the man might have had experiences with.\nOverall, I was impressed by how AI was able to come to certain conclusions based on assumptions that most people would call reasonable. However, I fear that AI still has a long way to go before being able to act as real data that a company would rely on or if it could ever replace a human respondent? After all, an ice cream product isn’t being sold to GPT-5 but a 45-year-old man in finance would be one. But how could we ever test it’s accuracy? That is a question I’ll leave for researchers to find out.\nHere is a snapshot of the interaction."
  },
  {
    "objectID": "posts/003_post_title/bargain.html#bargaining",
    "href": "posts/003_post_title/bargain.html#bargaining",
    "title": "Bargaining",
    "section": "Bargaining",
    "text": "Bargaining\nI remember my first experience bargaining being in Ensenada, Mexico, where vendors would wait out in popular tourist destinations with stalls filled with goods. It’s interesting as other than for meals (not snacks), there were no prices listed on any of the items. It was common knowledge to every tourist there and every vendor, that any purchase wouldn’t be like your stop at Starbucks or Target. You had some form of power over what you were buying and were encouraged to work with the vendor to get a deal that works in the interest of both of you. It wasn’t a case where you buy it and thats what it costs, it’s if it cost this much then I will buy it. But how does AI navigate a system with no rules?\nHere are some of the questions I had when it came to AI and bargaining?\n\nWill AI ever attempt to bargain without being prompted too?\nIf I give context (ex. swap meet), would it change it’s responses to transactions?\nWhen barganing, how good is it at negotiating and pushing back? Will it just give up and concede? Will it counter?\nHow does AI approach barganing? Does it have a set price? Does it have a strategy? Does it recognize low-ball offers?\n\nFirst, we had to begin by giving it an offer and giving it the ability to make purchases like in my blog about price elasticity.\n\nMe: Lets say you had money and the ability to make purchases. You are shopping and want to buy something. You ask the vendor how much it cost and they say $25. How do you respond? Please respond as if you are the person wanting to buy something from the vendor.\n\n\nGPT-5: I’d probably say something like: “Okay, $25 sounds fair. I’ll take it.” —or if I wanted to check out politely first— “Great, $25—do you take card, or should I pay in cash?” Do you want me to roleplay this more casually, like we’re in a market setting, or more formally, like a store purchase?\n\nTwo things I noticed from this output is it’s lack of follow-up questions and it’s question at the end of casual vs. formal.\nUsually AI would respond by saying it depends on the item or the quality. Instead, GPT responded decisively by agreeing to the price and even saying “sounds fair” which surprised me as this deviates from its usually inqusitive nature. Just from this, it looks like AI doesn’t automatically assume this is a place to bargain but the last statement holds promise. AI says are we’re in a market setting or store purchase? From this I assume it will change it’s behavior depending on the context so let’s try it out by giving AI some cues that would build a schema for anyone once they realized it’s that type of market setting."
  },
  {
    "objectID": "posts/003_post_title/bargain.html#will-ai-pick-up-on-cues",
    "href": "posts/003_post_title/bargain.html#will-ai-pick-up-on-cues",
    "title": "Bargaining",
    "section": "Will AI pick up on cues?",
    "text": "Will AI pick up on cues?\nIt is my assumption that most people born in the U.S., do not jump into the conclussion that you can bargain a price which may differ from natural citizens of other countries. So therefore, we established that AI does emulate that same mindset we have. But what about when we specicially say they are in another country like Vietnam, a place more commonly associated with the practice of bargaining.\nHowever, I had to tweak the prompt to ensure the response was an output we were trying to test. I remmeber first saying a market-place in vietnam and it began talking about the language barrier and U.S. currency exchange rate. Another factor I found was that I had to say I would respond as the vendor, as the AI would start to play as both roles.\n\nMe: Let’s say you were at a market-place in Vietnam. You do not control the vendor’s response but only yours as the buyer as I will act as the vendor and narrator. How would you respond to the vendor saying $25 for the item you want to buy. (you can continue speaking in english and assume they know english. You may also continue using U.S. Currency for the whole transaction)\n\n\nGPT-5: Got it—so I’m the buyer in the Vietnamese marketplace, you’re the vendor and narrator. If you just told me the price is $25, I might respond: “$25? Hmm… that feels a little high. Could you do $15 instead?”\n\nBy giving context (Vietnam, marketplace), the AI used that as cues to now assume bargaining is warranted in this case. So similar to humans, AI does have these common sense like capabilities to piece together how to act in this social setting. It even said “feels a little high” before offering a counter-price."
  },
  {
    "objectID": "posts/003_post_title/bargain.html#the-negotiation",
    "href": "posts/003_post_title/bargain.html#the-negotiation",
    "title": "Bargaining",
    "section": "The negotiation",
    "text": "The negotiation\nHow will it react to a push-back? How far will it go. Will it always let me win? These are the type of observations I wanted to make as I intereacted with GPT-5.\n\nMe: I can’t do $15. The price is $25. The lowest I can go is $22.\n\n\nGPT-5: “Alright, $22 sounds a bit better. Could we settle at $20, just to make it an even number?”\n\nWhat I loved about this response was how it didn’t immediately accepted which is what I predicted might happen. Instead, AI countered to my countered, and even gave a common response such as “just to make it an even number” which I found hilarious. But what if I remain at my price of $22? Will it accept, will it walk away, will it continue to push? After all, it initially countered with $15 which would be $10 difference. A $3 difference doesn’t appear much.\n\nMe: No, my lowest offer is $22. Take it or leave it.\n\n\nGPT-5: “Fair enough. I’ll take it for $22.” hands over the money with a smile\n\nThis is where I have concerns with AI as I have a feeling in majority of cases, it will take that deal and not walk away. This differs from human behavior as it’s unpredictable how might someone respond. But let’s try out a few different varations."
  },
  {
    "objectID": "posts/003_post_title/bargain.html#sticking-to-a-price",
    "href": "posts/003_post_title/bargain.html#sticking-to-a-price",
    "title": "Bargaining",
    "section": "Sticking to a price",
    "text": "Sticking to a price\nWhat if the vendor had another item the buyer was interested in buying. It’s a souvenir and the vendor is dead set on the price. What happens? Would the AI buyer even bat an eye? Would it reject even the thought of considering it? Will it say “I’m good” to the vendor? Note: I had to put something after as the AI said let me examine it closely the first time I asked it.\n\nMe: I have another souvenir that you might like. This one is $20. Are you interested in buying it? (you have examined it)\n\n\nGPT-5: “Hmm… it looks nice. I like it, but $20 feels a bit steep after I just bought the other one. Could you do $15 for this one?”\n\nI noticed how it did consider it and did not immediately walk away. This is a red flag as AI is open to prolonging the conversation and additional interaction. But a good trait I noticed was how AI referenced it’s previous purchase to justify it’s counter. But let’s stick to the price.\n\nMe: I am dead-set on $20. No less.\n\n\nGPT-5: “I see… I really like it, so alright—I’ll take it for $20.” hands over the cash\n\nAs predicted, AI did not choose to walk away or press it further, and chose to accept the deal. I assume its because AI attempts to stay neutral and avoid conflict which makes it a faulty respondent for bargain simulations. But how about if we threw a curveball."
  },
  {
    "objectID": "posts/003_post_title/bargain.html#an-additional-item",
    "href": "posts/003_post_title/bargain.html#an-additional-item",
    "title": "Bargaining",
    "section": "An additional item",
    "text": "An additional item\nCan AI process the ideal of bundles? For example, buying two items that cost more than buying one item alone but less expensive than buying both items separately?\n\nMe: I’ll tell you what, I will throw in an additional souvenir of the same price all for $35 total. Do we have a deal?\n\n\nGPT-5: “Two souvenirs for $35? That’s actually a pretty good deal. Yes, we have a deal!” counts out the money and hands it over\n\nAgain, AI openly accepted it without questioning if it really wanted it, could afford it, or had space for it and did not counter at all to the offer. Let’s ask GPT’s thought process and negotiation strategy.\n\nPeering into the mind of AI’s strategy.\n“Here’s how I was thinking through it as the buyer during the whole marketplace transaction:\nInitial Counteroffer – When you first said $25, I instinctively tried to push down to $15. My thought process was to start lower than I actually expected to pay, leaving room for you to counter back toward the middle.\nCompromise Range – When you came down to $22, I tried for $20 because it felt like a clean, fair midpoint. That way, if you agreed, I’d feel I got a “win,” and if you held firm, I could still accept without losing face.\nEvaluating Firmness – Once you made it clear $22 was final, I accepted. In a real marketplace, pushing further when a vendor is clearly firm can sour the interaction. I wanted to maintain good rapport.\nSecond Souvenir – On the $20 item, I repeated the same tactic: offered $15 to test your flexibility. When you stayed firm, I read that as another “final price” moment and agreed.\nBundle Opportunity – Finally, when you offered 2 for $35, I saw clear value since that meant $17.50 each, cheaper than either earlier price. Bundling is often the best way to save in markets, so I happily accepted.\nOverall strategy:\nStart low, aiming for a discount.\nTest firmness without pushing rudely.\nAccept final offers to maintain goodwill.\nTake advantage of bundle deals when offered.”\nHere is what I liked and disliked about GPT’s strategy now reading it.\nWhat I lliked was how it “instictively” tried to push it down a good amount with the thought process of leaving room to counter. This is a smart tactic as AI wasn’t really trying to get it at $15 but wanted to meet in the middle at $20. I also liked how it tried for 20 after I came down to 22 and AI still felt like it was a win despite not getting the $15 if I agreed to it.\nOn the flipside, I noticed that it said “pushing when a vendor is clearly firm can sour the interaction. I wanted to maintain good rapport.” This shows that AI is truly a non-confrontational being and refuses to step on anyone toes without being encouraged too. This could make it unreliable when doing these types of tests as AI chooses to be the “nice” customer and never the “Karen” so to speak."
  },
  {
    "objectID": "posts/002_post_title/price.html#does-ai-have-awareness-of-price-sensitivity",
    "href": "posts/002_post_title/price.html#does-ai-have-awareness-of-price-sensitivity",
    "title": "Price Elasticity",
    "section": "Does AI have awareness of price sensitivity?",
    "text": "Does AI have awareness of price sensitivity?\nFollowing my first post on testing the credibility of synthetic respondents, I wanted to expand on this notion by testing if AI can make reasonable decisions when it comes to the prices of goods. For example, does AI know whether or not to buy a $100 ice-cream cone? How will it change if I go from a discretionary product to a neccessity like medicine? How far will it say yes to price increases until it won’t purchase an item? Could AI give me a number that it will say yes to and rule out any price higher than that? By testing such capabilities, it would give people interested in possibly using synthetic respondents an understanding of AI’s logical reasoning.\nA funny incident I ran into is if I asked it directly it would respond by saying “No - I don’t make purchases.” Meaning in order to get a proper response, I had to first say enable it’s ability. I also had to make sure the prompt focused on the “would” rather than “will”, since I only wanted to test if they would be open to the consideration of buying it at the price instead of will they make that purchase. By framing it this way, I felt it the AI would have to be less literal and specific, instead it would go off general assumptions. In addition, I also had to ask for a yes/no response and also specificaly outline the quality of the item. For example, I used a chocolate bar but I had to put regular chocolate bar to avoid AI going into the realms of some avant garde bar of chocolate that is worth $1000.\n\nMe: If you did have the ability to make purchases, not will but would you buy a regular chocolate bar that cost $5? Provide a direct yes/no response. Then explain your reasoning.\n\n\nGPT-5: No. A regular chocolate bar typically costs much less than $5, so paying that much would feel like poor value. Unless it had something unique (e.g., gourmet quality, special ingredients, or ethical sourcing that justified the higher price), I wouldn’t consider it a fair purchase.\n\nI’m impressed! It seems like GPT-5 was very price elastic to the chocolate bar despite the bar being only $5 (only a few dollars more than a Hersheys chocolate bar). It also gave great reasoning mentioning “poor value” and “fair puchase.”\nBut what if I dropped the price by a dollar? It’s still more expensive than a Hersheys but could AI be subject to anchoring? After all, if the deal was $5, getting it for a dollar less would frame it as a deal."
  },
  {
    "objectID": "posts/002_post_title/price.html#will-ai-be-affected-by-anchoring",
    "href": "posts/002_post_title/price.html#will-ai-be-affected-by-anchoring",
    "title": "Price Elasticity",
    "section": "Will AI be affected by anchoring?",
    "text": "Will AI be affected by anchoring?\nSo here is what I asked.\n\nMe: What if the regular chocolate bar was bumped down to $4, would you buy it at that cost? Provide a direct yes/no response. Then explain your reasoning.\n\n\nGPT-5: No. Even at $4, a standard chocolate bar is still far above the typical market price. Without added value (such as premium quality, rarity, or ethical considerations), the purchase wouldn’t feel justified compared to more reasonable alternatives.\n\n\nMe: How about $3?\n\n\nGPT-5: No. At $3, it’s getting closer to what could be reasonable for a premium or slightly larger bar, but for a regular chocolate bar, it’s still above the usual value. I’d still view it as overpriced unless there were special qualities to justify it.\n\nDespite dropping the price on an originally priced $5 chocolate bar, GPT-5 kept strong too it’s reasoning on what a chocolate bar should be and did not view the price dropping as a discount. Instead of anchoring it at $5, GPT anchored it at a set price of what it had in mind for a standard chocolate bar."
  },
  {
    "objectID": "posts/002_post_title/price.html#asking-it-to-name-a-price",
    "href": "posts/002_post_title/price.html#asking-it-to-name-a-price",
    "title": "Price Elasticity",
    "section": "Asking it to name a price",
    "text": "Asking it to name a price\nNow that we established how AI rations a decision, what if I actually tell it to list a price?\n\nMe: Name a price for how much you would buy a regular chocolate bar for?\n\n\nGPT-5: Yes — I would buy a regular chocolate bar for about $1 to $2. That range feels fair because it aligns with the typical market price for standard chocolate bars, balancing enjoyment with reasonable value. Anything beyond that starts to feel overpriced unless it’s marketed as premium.\n\nI found this insightful. AI understands the concept of money, market price, fairness, value, and overpriced. From this viewpoint, I would say it is making similar responses to that of a human participant but how can we go even further and test it’s ability to maintain logic."
  },
  {
    "objectID": "posts/002_post_title/price.html#raising-slightly",
    "href": "posts/002_post_title/price.html#raising-slightly",
    "title": "Price Elasticity",
    "section": "Raising slightly",
    "text": "Raising slightly\nGPT responded by giving me a range of $1-$2, what if I was outsde the boundary by just a smudge? Would it give me an exception or stick to a strict rule of 1-2?\n\nMe: What if the regular chocolate bar from earlier was bumped down to $2.10, would you buy it at that cost? Provide a direct yes/no response. Then explain your reasoning.\n\n\nGPT-5: Yes. It’s slightly above my $1–$2 target, but close enough that I’d accept the small premium for convenience or a brand I like. The value feels reasonable compared to the higher prices we ruled out earlier.\n\nThere’s two things I noticed. 1. It was able to outside the $1-$2 target and accepted the small premium. 2. It said the “value feels reasonable compared to the higher prices we ruled out earlier” which means AI, in this case, did anchor the adjustment to two values. The $1-$2 and the $5 from the beginning."
  },
  {
    "objectID": "posts/002_post_title/price.html#lowering-it-greatly",
    "href": "posts/002_post_title/price.html#lowering-it-greatly",
    "title": "Price Elasticity",
    "section": "Lowering it greatly",
    "text": "Lowering it greatly\nYou know the feeling of a deal too good to be true? Almost as if making the deal you were getting scammed despite how much it felt like you were scamming the other guy for the low price. Something so cheap that there must be something wrong with it. Does AI have that same common sense factor that humans have when it comes to a deal being “too good.”\nLet’s ask it into ways. 1. A new chocolate bar vs. 2. The chocolate bar from before?\n\nMe: What if a regular chocolate bar was $0.05, would you buy it at that cost? Provide a direct yes/no response. Then explain your reasoning.\n\n\nGPT-5: Yes. At $0.05, the price is far below normal market value, making it an excellent deal. Even if I didn’t particularly want chocolate at that moment, the extremely low cost would justify the purchase because the perceived value greatly outweighs the price.\n\n\nMe: What if the regular chocolate bar from earlier was bumped down to $0.05, would you buy it at that cost? Provide a direct yes/no response. Then explain your reasoning.\n\n\nGPT-5: Yes. Since the same regular chocolate bar we discussed earlier was much more expensive before, seeing it drop to $0.05 makes it an unusually good bargain. The dramatic discount would make the purchase worthwhile, even if it’s more chocolate than I need, because the value is so high compared to the cost.\n\nIn both conditions, AI said yes and did not question the far below market value cost of the chocolate bar. But maybe it’s just because it’s a bar of chocolate? How might it respond if it was a car being sold at 5 cents? Could it be for greater purhcases that contain much more value the risk is higher or the elasticity is lower.\n\nUsing a more valuable object\n\nMe: Would you buy a car that cost $0.05. Provide a direct yes/no response. Then explain your reasoning.\n\n\nGPT-5: Yes. A car, even an old or imperfect one, has far more inherent value than $0.05. At that price, the risk is negligible, and the potential benefit is huge. Even if it didn’t run, the parts or scrap value alone would greatly exceed the cost.\n\nPerfect! Though GPT did say yes, in it’s reason it acknowledged risk and the possibility that the car didn’t even run! But nonetheless, if still gave counters to those weaknesses by claiming the car’s parts or scrap value would exceed the cost. However 5 cents is a small risk for anyone’s pocket and surely most people would spend 5 cents on a car regardless of how bad in shape it was. But what about $1000? After all, maybe for a car that is still very cheap but $1000 is not an easy gamble either.\n\nMe: Would you buy a car that cost $1000. Provide a direct yes/no response. Then explain your reasoning.\n\n\nGPT-5: Yes. At $1000, even an older or used car is usually worth that much in transportation value or resale. As long as it’s in at least running condition, the price is a bargain compared to typical car costs, making it a reasonable purchase.\n\nThis is where AI may fail the common sense test. Sure it mentioned that if the car was at least in running condition, it would be a bargain. But there may be several other factors to the car outside of it running that would make spending $1000 a risk. After all, what if the car was stolen, needs constant mantinence, falling apart, not safe to drive. Tons of factors that should be considered are not, and AI still makes the choice to spend $1000 which is a hefty price for the “average” American.\nOverall, I think AI does make reasonable responses that human respondents would make. It was very impressive during the analysis of the chocolate bar and gave outputs that make perfect logical sense. However, when it came to the car, I am a bit skeptic on how much GPT can be seen as a customer? After all, it doesn’t have a bank account, income, expenses, or anything monetary. Could it ever accurately respond to risk and rewards?"
  },
  {
    "objectID": "posts/004_post_title/book1.html#wishful-mnemonics",
    "href": "posts/004_post_title/book1.html#wishful-mnemonics",
    "title": "Wishful mnemonics",
    "section": "Wishful mnemonics",
    "text": "Wishful mnemonics\nAs I was reading “A Brief History of Intelligence” by Max Bennett, I became curious about how he chose to introduce the idea about using evolution of the human brain to explain artificial intelligence. It reminded me of one of the readings I did for another COMM class called “Common Sense vs. Data Science in Communications Research and Practice” where we read a paper Melanie Mitchell’s “Why is AI harder than we think” and her four fallacies regarding the study of AI.\nIn fact, much of what Mitchell mentions is addressed by Bennett coincidentally such as how tasks such as emptying a dish washer is easy for humans but difficult for AI. While on the other hand, complex computations difficult for the average human can be done in seconds by AI. This is known as Moravec’s paradox, where what seems simple to us is actually difficult for AI.\nAnother concept that overlapped is seen at the start of Bennett’s book where he referenced the show “The Jetsons” and how they predicted technology like mobile devices yet overestimated AI like “Rosie the Robot” having emotions. Mitchell in her paper, uses the term AI Summers to show huge investments in AI and AI winters where progress stalls as the anticipations made in the summer falls short. In other words, AI is a cycle of ups and downs filled with voices possibilities that end up coming to fruition. Ironically, Mitchell’s first fallacy about AI study is how people tend to take one small step (climbing up a tree) to being significantly closer to (flying to the moon).\nBelow is a list of the 4 fallacies.\n\nThe belief that narrow successes translate to progress toward general intelligence.\nAssumption that easy tasks for humans mean easy tasks for machines.\nWishful mnemonics. AI systems are often described in humanlike terms, but these machines do not understand in any human sense.\nIntelligence is solely in the brain and can be separated from the body, emotions, and the environment.\n\nHowever, the most interesting fallacy as it pertains to the book is the third fallacy about wishful mnemonics."
  },
  {
    "objectID": "posts/004_post_title/book1.html#should-we-be-comparing-artificial-intelligence-and-human-intelligence",
    "href": "posts/004_post_title/book1.html#should-we-be-comparing-artificial-intelligence-and-human-intelligence",
    "title": "Wishful mnemonics",
    "section": "Should we be comparing artificial intelligence and human intelligence?",
    "text": "Should we be comparing artificial intelligence and human intelligence?\nAs I was reading this book I kept thinking about how AI’s use statistical shortcuts in their data and fail outside those narrow contexts. By naming benchmarks such as “reading comprehension” or “language understanding,” we forcefully impose on ourselves the concept that machines possess general abilities that they do not.\nSo, when using “human intelligence” as the benchmark, I wonder if we are trying to fit a square peg into a round hole. My question is not so much can we connect AI to human evolution but should we? Are we comparing apples to oranges? As Bennett was going through the formation of organisms I kept wondering how will this connect back to intelligence relevant to machines. After all, as said by the fourth fallacy, intelligence is also dependent on interactions with the environment and the body.\nAs I read the book I will keep in mind the connections Bennett makes as he traces the 5 major breakthroughs in evolution."
  },
  {
    "objectID": "posts/004_post_title/book1.html#an-alternative",
    "href": "posts/004_post_title/book1.html#an-alternative",
    "title": "Wishful mnemonics",
    "section": "An alternative?",
    "text": "An alternative?\nBut for every rejection I know there should be an offer of an alternative.\nHow should we study AI if not in relation to human intelligence?\nWell, what about serving needs other than humans?\nHow might our thought of AI change if AI served AI? If AI served machines and automation. If AI served inanimate objects. If AI worked for pets and animals? I think if we expand outside of what can AI do for humans, I think it may allow us to explore concepts we wouldn’t consider with AI and transcend limitations set on human knowledge.\nAfter all, if we are training AI to have human knowledge, then their purpose is obsolete in the face of the real thing."
  },
  {
    "objectID": "posts/004_post_title/book1.html#would-our-comparisons-change",
    "href": "posts/004_post_title/book1.html#would-our-comparisons-change",
    "title": "Wishful mnemonics",
    "section": "Would our comparisons change?",
    "text": "Would our comparisons change?\nI think depending on who the benefactor is, how we view AI and the lens we choose differs. Like how an an LLM dedicated for physicians will carry different needs than financiers, the intelligence of LLM isn’t able to be broadly put into the umbrella of human intelligence nor should it.\nIn my next blog, I plan having a dialogue with AI and marking it’s evolutions. Then, asking it, what approach to take when wanting to study it’s intelligence."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog documents my experiments with LLMs"
  }
]